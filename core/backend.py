import queue
import numpy as np
import gtsam
from gtsam.symbol_shorthand import X, V, B, L
from gtsam_unstable import IncrementalFixedLagSmoother, FixedLagSmootherKeyTimestampMap

import re
from utils.debug import Debugger
import time

class Backend:
    def __init__(self, global_central_map, config, imu_processor):
        self.global_central_map = global_central_map
        self.config = config

        # ä½¿ç”¨ iSAM2 ä½œä¸ºä¼˜åŒ–å™¨
        self.lag_window_size = config.get('lag_window_size', 10) # ä¼˜åŒ–å™¨çš„æ»‘çª—
        parameters = gtsam.ISAM2Params()
        parameters.setRelinearizeThreshold(0.01) 
        parameters.relinearizeSkip = 1
        self.smoother = IncrementalFixedLagSmoother(self.lag_window_size, parameters) # è‡ªåŠ¨è¾¹ç¼˜åŒ–
        
        # é²æ£’å› å­
        self.visual_noise = gtsam.noiseModel.Isotropic.Sigma(2, 3.0)
        self.visual_robust_noise = gtsam.noiseModel.Robust.Create(gtsam.noiseModel.mEstimator.Huber.Create(1.345), self.visual_noise)

        # çŠ¶æ€ä¸idç®¡ç†
        self.kf_id_to_gtsam_id = {}
        self.landmark_id_to_gtsam_id = {}
        self.next_gtsam_kf_id = 0
        self.factor_indices_to_remove = gtsam.KeyVector()

        # è·å–ç›¸æœºå†…ã€å¤–å‚
        cam_intrinsics = np.asarray(self.config.get('cam_intrinsics')).reshape(3, 3)
        self.K = gtsam.Cal3_S2(cam_intrinsics[0, 0], cam_intrinsics[1, 1], 0, 
                               cam_intrinsics[0, 2], cam_intrinsics[1, 2])

        T_bc_raw = self.config.get('T_bc', np.eye(4).flatten().tolist())
        self.T_bc = np.asarray(T_bc_raw).reshape(4, 4)
        self.body_T_cam = gtsam.Pose3(self.T_bc)
        self.cam_T_body = self.body_T_cam.inverse()

        # å­˜å‚¨æœ€æ–°çš„ä¼˜åŒ–åçš„åç½®ï¼Œç”¨äºIMUé¢„ç§¯åˆ†
        self.latest_bias = gtsam.imuBias.ConstantBias()

        # å®šä¹‰è¦è®°å½•çš„åˆ—
        log_columns = [
            "gtsam_id", "pos_x", "pos_y", "pos_z",
            "vel_x", "vel_y", "vel_z",
            "bias_acc_x", "bias_acc_y", "bias_acc_z",
            "bias_gyro_x", "bias_gyro_y", "bias_gyro_z",
            "new_factors_error"
        ]
        # åˆå§‹åŒ–Debugger
        self.logger = Debugger(file_prefix="backend_state", column_names=log_columns)

    # å…³é”®å¸§idæ˜ å°„åˆ°å›¾çš„id
    def _get_kf_gtsam_id(self, kf_id):
        if kf_id not in self.kf_id_to_gtsam_id:
            self.kf_id_to_gtsam_id[kf_id] = self.next_gtsam_kf_id
            self.next_gtsam_kf_id += 1
        return self.kf_id_to_gtsam_id[kf_id]

    # è·¯æ ‡ç‚¹idæ˜ å°„åˆ°å›¾çš„id
    def _get_lm_gtsam_id(self, lm_id):
        if lm_id not in self.landmark_id_to_gtsam_id:
            self.landmark_id_to_gtsam_id[lm_id] = lm_id
        return self.landmark_id_to_gtsam_id[lm_id]

    def get_latest_optimized_state(self):
        if self.next_gtsam_kf_id == 0:
            return None, None, None
        
        latest_gtsam_id = self.next_gtsam_kf_id - 1

        result = self.smoother.calculateEstimate()

        try:
            pose = result.atPose3(X(latest_gtsam_id))
            velocity = result.atVector(V(latest_gtsam_id))
            bias = result.atConstantBias(B(latest_gtsam_id))
            # print(f"ã€Backendã€‘: Latest optimized state: pose: {pose.matrix()}, velocity: {velocity}, bias: {bias}")
            return pose, velocity, bias
        except Exception as e:
            print(f"[Error][Backend] Failed to retrieve latest state for gtsam_id {latest_gtsam_id}: {e}")
            return None, None, None

    def update_estimator_map(self, keyframe_window, landmarks):
        print("ã€Backendã€‘: Syncing optimized results back to Estimator...")
        optimized_results = self.smoother.calculateEstimate()

        # æ›´æ–°å…³é”®å¸§ä½å§¿
        for kf in keyframe_window:
           # è·å–å¾…æ›´æ–°å…³é”®å¸§çš„gtsam_id
            gtsam_id = self.kf_id_to_gtsam_id.get(kf.get_id())
            if gtsam_id is not None and optimized_results.exists(X(gtsam_id)):
                
                # ä»ä¼˜åŒ–ç»“æœä¸­è·å–æœ€æ–°çš„IMUä½å§¿ T_w_b
                pose_w_b = optimized_results.atPose3(X(gtsam_id))
                
                # è½¬æ¢å›ç›¸æœºä½å§¿ T_w_c å¹¶æ›´æ–°
                pose_w_c = pose_w_b.compose(self.body_T_cam)
                kf.set_global_pose(pose_w_c.matrix())

        # æ›´æ–°è·¯æ ‡ç‚¹åæ ‡
        for lm_id, landmark_obj in landmarks.items():
            gtsam_id = self._get_lm_gtsam_id(lm_id)
            if gtsam_id is not None and optimized_results.exists(L(gtsam_id)):
                # 1. ä»ä¼˜åŒ–ç»“æœä¸­è·å–æœ€æ–°çš„3Dåæ ‡
                optimized_position = optimized_results.atPoint3(L(gtsam_id))
                # 2. è°ƒç”¨å¯¹è±¡çš„æ–¹æ³•æ¥æ›´æ–°å…¶å†…éƒ¨çŠ¶æ€
                landmark_obj.set_triangulated(optimized_position)
                # print(f"ã€Backendã€‘: Updated landmark {lm_id} to {optimized_position}")

    def remove_stale_landmarks(self, unhealty_lm_ids, unhealty_lm_ids_depth, oldest_kf_id_in_window):
        # 2. ä¿®æ”¹ï¼šè¿™ä¸ªå‡½æ•°ç°åœ¨åªè´Ÿè´£â€œç™»è®°â€è¦åˆ é™¤çš„è·¯æ ‡ç‚¹IDï¼Œä¸è®¡ç®—ç´¢å¼•
        print(f"ã€Backendã€‘: æ¥æ”¶åˆ°ç§»é™¤ {len(unhealty_lm_ids)} ä¸ªé™ˆæ—§è·¯æ ‡ç‚¹çš„æŒ‡ä»¤ã€‚")
        if not unhealty_lm_ids:
            return

        graph = self.smoother.getFactors()
        factor_indices_to_remove = gtsam.KeyVector()
        unhealty_lm_keys = {L(self._get_lm_gtsam_id(lm_id)) for lm_id in unhealty_lm_ids}

        factor_indices_to_remove_depth = gtsam.KeyVector()
        unhealty_lm_keys_depth = {L(self._get_lm_gtsam_id(lm_id)) for lm_id in unhealty_lm_ids_depth}

        # oldest_kf_id_in_window += 1 
        oldest_gtsam_key = None
        if oldest_kf_id_in_window is not None and oldest_kf_id_in_window in self.kf_id_to_gtsam_id:
            oldest_gtsam_key = X(self._get_kf_gtsam_id(oldest_kf_id_in_window))
            print(f"ã€Backendã€‘: æœ€æ—§çš„å…³é”®å¸§çš„gtsam_id: {oldest_gtsam_key}")


        for i in range(graph.size()):
            factor = graph.at(i)
            if factor is not None:
                for key in factor.keys():
                    # print(f"key: {key}")
                    # if key == oldest_gtsam_key:
                    #     factor_type = factor.__class__.__name__
                    #     key_str = ", ".join([gtsam.DefaultKeyFormatter(key) for key in factor.keys()])
                    #     print(f"  [è·³è¿‡åˆ é™¤] Index: {i}, ç±»å‹: {factor_type}, è¿æ¥: [{key_str}] (è¿™æ˜¯æœ€æ—§çš„å…³é”®å¸§)")
                    #     break # ç»ä¸åˆ é™¤ä¸æœ€æ—§çš„å…³é”®å¸§ç›¸è¿çš„å› å­

                    if key in unhealty_lm_keys:
                        factor_type = factor.__class__.__name__
                        key_str = ", ".join([gtsam.DefaultKeyFormatter(key) for key in factor.keys()])
                        print(f"  [æ ‡è®°åˆ é™¤] Index: {i}, ç±»å‹: {factor_type}, è¿æ¥: [{key_str}]")
                        factor_indices_to_remove.append(i)

                        if key in unhealty_lm_keys_depth:
                            print(f"æ£€æµ‹åˆ°æ·±åº¦ä¸ºè´Ÿçš„å› å­ï¼Œå°è¯•åˆ é™¤")
                            # print(f"key: {key}")
                            print(f"  [æ ‡è®°åˆ é™¤æ·±åº¦] Index: {i}, ç±»å‹: {factor_type}, è¿æ¥: [{key_str}]")
                            if factor_type != 'GenericProjectionFactorCal3_S2':
                                print(f"  [è·³è¿‡åˆ é™¤] Index: {i}, ç±»å‹: {factor_type} (è¿™æ˜¯è¾¹ç¼˜åŒ–é”šç‚¹)")
                                continue # ç»ä¸åˆ é™¤è¾¹ç¼˜åŒ–å› å­
                            
                            factor_indices_to_remove_depth.append(i)
                            print(f"  [ç¡®è®¤åˆ é™¤æ·±åº¦] Index: {i}, ç±»å‹: {factor_type}, è¿æ¥: [{key_str}]")
                        break
        
        self.factor_indices_to_remove = factor_indices_to_remove_depth

        if factor_indices_to_remove_depth:
            empty_graph = gtsam.NonlinearFactorGraph()
            empty_values = gtsam.Values()
            empty_stamps = FixedLagSmootherKeyTimestampMap()
            self.smoother.update(empty_graph, empty_values, empty_stamps, factor_indices_to_remove_depth)
            print(f"ã€Backendã€‘: æˆåŠŸç§»é™¤ {len(factor_indices_to_remove_depth)} ä¸ªæ·±åº¦ä¸ºè´Ÿçš„è·¯æ ‡ç‚¹çš„å› å­")

        for lm_id in unhealty_lm_ids:
            if lm_id in self.landmark_id_to_gtsam_id:
                del self.landmark_id_to_gtsam_id[lm_id]

        print(f"ã€Backendã€‘: æˆåŠŸç§»é™¤ {len(unhealty_lm_ids)} ä¸ªè·¯æ ‡ç‚¹çš„å› å­")
        

    def initialize_optimize(self, initial_keyframes, initial_imu_factors, initial_landmarks, initial_velocities, initial_bias):
        print("ã€Backendã€‘: Initializing optimize...")

        graph = gtsam.NonlinearFactorGraph()
        estimates = gtsam.Values()
        
        initial_window_stamps = FixedLagSmootherKeyTimestampMap()

        for i, kf in enumerate(initial_keyframes):
            kf_gtsam_id = self._get_kf_gtsam_id(kf.get_id())

            # ä»åˆå§‹åŒ–ç»“æœä¸­è·å–ä½å§¿ã€é€Ÿåº¦å’Œåç½®
            T_wc = gtsam.Pose3(kf.get_global_pose())
            T_wb = T_wc.compose(self.cam_T_body)
            # initial_velocities æ˜¯ä¸€ä¸ªæ‰å¹³åŒ–çš„æ•°ç»„ï¼Œæ¯3ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªé€Ÿåº¦å‘é‡
            velocity = initial_velocities[i*3 : i*3+3]
            
            # æ‰€æœ‰å¸§ä½¿ç”¨ç›¸åŒçš„åˆå§‹åç½®
            bias = initial_bias

            # æ·»åŠ åˆå§‹ä¼°è®¡å€¼
            estimates.insert(X(kf_gtsam_id), T_wb)
            estimates.insert(V(kf_gtsam_id), velocity)
            estimates.insert(B(kf_gtsam_id), bias)

            # æ·»åŠ æ»‘çª—è®°å½•
            initial_window_stamps.insert((X(kf_gtsam_id), float(kf_gtsam_id)))
            initial_window_stamps.insert((V(kf_gtsam_id), float(kf_gtsam_id)))
            initial_window_stamps.insert((B(kf_gtsam_id), float(kf_gtsam_id)))

            # ä¸ºæ¯ä¸€ä¸ªlandmarkè®¾ç½®æ»‘çª—è®°å½•
            last_gtsam_id = self._get_kf_gtsam_id(initial_keyframes[-1].get_id())
            for lm_id in initial_landmarks.keys():
                lm_gtsam_id = self._get_lm_gtsam_id(lm_id)
                initial_window_stamps.insert((L(lm_gtsam_id), float(last_gtsam_id))) # è®¾ä¸ºæœ€åä¸€å¸§çš„ID

            # ä¸ºç¬¬ä¸€å¸§æ·»åŠ å¼ºå…ˆéªŒ
            if kf_gtsam_id == 0:
                prior_pose_noise = gtsam.noiseModel.Diagonal.Sigmas(np.array([1e-4]*3 + [1e-2]*3))
                prior_vel_noise = gtsam.noiseModel.Diagonal.Sigmas(np.array([1e-2] * 3))
                prior_bias_noise = gtsam.noiseModel.Diagonal.Sigmas(np.array([1e-1]*3 + [1e-2]*3))
                graph.add(gtsam.PriorFactorPose3(X(0), T_wb, prior_pose_noise))
                graph.add(gtsam.PriorFactorVector(V(0), velocity, prior_vel_noise))
                graph.add(gtsam.PriorFactorConstantBias(B(0), bias, prior_bias_noise))
            elif kf_gtsam_id == len(initial_keyframes) - 1:
                # ä¸ºæœ€åä¸€å¸§æ·»åŠ è¾ƒå¼±çš„ä½ç½®å…ˆéªŒä»¥ç¨³å®šå°ºåº¦
                prior_pose_noise = gtsam.noiseModel.Diagonal.Sigmas(np.array([0.1]*3 + [0.5]*3))
                graph.add(gtsam.PriorFactorPose3(X(kf_gtsam_id), T_wb, prior_pose_noise))

        # æ·»åŠ æ‰€æœ‰åˆå§‹IMUå› å­
        for factor_data in initial_imu_factors:
            start_kf = next(kf for kf in initial_keyframes if kf.get_timestamp() == factor_data['start_kf_timestamp'])
            end_kf = next(kf for kf in initial_keyframes if kf.get_timestamp() == factor_data['end_kf_timestamp'])
            gtsam_id1 = self._get_kf_gtsam_id(start_kf.get_id())
            gtsam_id2 = self._get_kf_gtsam_id(end_kf.get_id())
            pim = factor_data['imu_preintegration']
            graph.add(gtsam.CombinedImuFactor(X(gtsam_id1), V(gtsam_id1), X(gtsam_id2), V(gtsam_id2), B(gtsam_id1), B(gtsam_id2), pim))

        # æ·»åŠ æ‰€æœ‰åˆå§‹è·¯æ ‡ç‚¹å˜é‡å’Œè§†è§‰å› å­
        for lm_id, lm_3d_pos in initial_landmarks.items():
            lm_gtsam_id = self._get_lm_gtsam_id(lm_id)
            estimates.insert(L(lm_gtsam_id), lm_3d_pos)

        for kf in initial_keyframes:
            kf_gtsam_id = self._get_kf_gtsam_id(kf.get_id())
            for lm_id, pt_2d in zip(kf.get_visual_feature_ids(), kf.get_visual_features()):
                # åªå¤„ç†æœ¬æ¬¡ä¼˜åŒ–ä¸­æ–°æ·»åŠ çš„landmark
                if lm_id in initial_landmarks:
                    lm_gtsam_id = self._get_lm_gtsam_id(lm_id)
                    factor = gtsam.GenericProjectionFactorCal3_S2(pt_2d, self.visual_robust_noise, X(kf_gtsam_id), L(lm_gtsam_id), self.K, body_P_sensor=self.body_T_cam)
                    graph.add(factor)

        # æ‰§è¡ŒiSAM2çš„ç¬¬ä¸€æ¬¡æ›´æ–°ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰
        print(f"ã€Backendã€‘: Initializing iSAM2 with {graph.size()} new factors and {estimates.size()} new values...")
        
        try:
            start_time = time.time()
            self.smoother.update(graph, estimates, initial_window_stamps)
            end_time = time.time()
            print(f"ã€Backend Timerã€‘: Initial optimization took { (end_time - start_time) * 1000:.3f} ms.")
        except RuntimeError as e:
            print("\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
            print("!!!!!!!!!! INITIALIZATION FAILED !!!!!!!!!!!!!!")
            print(f"ERROR: {e}")
            return # å¤±è´¥æ—¶å¿…é¡»è¿”å›

        # æ›´æ–°æœ€æ–°bias
        latest_pose, latest_vel, latest_bias = self.get_latest_optimized_state()
        print(f"ã€Backendã€‘: Latest optimized state: pose: {latest_pose.matrix()}, velocity: {latest_vel}, bias: {latest_bias}")

        latest_gtsam_id = self.next_gtsam_kf_id - 1
        print(f"ã€Backendã€‘: Latest gtsam_id: {latest_gtsam_id}")
        if latest_bias is not None:
            self.latest_bias = latest_bias
        print("ã€Backendã€‘: Initial graph optimization complete.")

        # è®°å½•ä¼˜åŒ–çŠ¶æ€
        new_factors_error = self._log_optimization_error(graph)
        self._log_state_and_errors(latest_gtsam_id, latest_pose, latest_vel, latest_bias, new_factors_error)


    def optimize_incremental(self, last_keyframe, new_keyframe, new_imu_factors, 
                            new_landmarks, new_visual_factors, initial_state_guess, is_stationary, oldest_kf_id_in_window):

        new_graph = gtsam.NonlinearFactorGraph()
        new_estimates = gtsam.Values()
        new_window_stamps = FixedLagSmootherKeyTimestampMap()

        # æ·»åŠ æ–°å…³é”®å¸§çš„çŠ¶æ€å˜é‡ï¼Œä½¿ç”¨IMUé¢„æµ‹å€¼ä½œä¸ºåˆå§‹ä¼°è®¡
        kf_gtsam_id = self._get_kf_gtsam_id(new_keyframe.get_id())
        T_wb_guess, vel_guess, bias_guess = initial_state_guess

        new_estimates.insert(X(kf_gtsam_id), T_wb_guess)
        new_estimates.insert(V(kf_gtsam_id), vel_guess)
        new_estimates.insert(B(kf_gtsam_id), bias_guess)

        # æ·»åŠ æ»‘çª—è®°å½•
        new_window_stamps.insert((X(kf_gtsam_id), float(kf_gtsam_id)))
        new_window_stamps.insert((V(kf_gtsam_id), float(kf_gtsam_id)))
        new_window_stamps.insert((B(kf_gtsam_id), float(kf_gtsam_id)))

        # æ·»åŠ IMUå› å­
        last_kf_gtsam_id = self._get_kf_gtsam_id(last_keyframe.get_id())
        pim = new_imu_factors['imu_preintegration']
        imu_factor = gtsam.CombinedImuFactor(
            X(last_kf_gtsam_id), V(last_kf_gtsam_id), X(kf_gtsam_id), V(kf_gtsam_id),
            B(last_kf_gtsam_id), B(kf_gtsam_id), pim)
        new_graph.add(imu_factor)

        # æ·»åŠ æ–°è·¯æ ‡ç‚¹é¡¶ç‚¹ï¼Œæ³¨æ„è¿™é‡Œæ·»åŠ çš„é¡¶ç‚¹åªåœ¨new_estimatesä¸­è¿˜æ²¡æœ‰è¿›å…¥isam2çš„å›¾
        for lm_id, lm_3d_pos in new_landmarks.items():
            lm_gtsam_id = self._get_lm_gtsam_id(lm_id)
            # ---!!!--- åœ¨æ­¤å¤„æ·»åŠ æ‚¨è¦çš„æ—¥å¿— ---!!!---
            # æ‰“å°å³å°†é€å…¥ä¼˜åŒ–å™¨çš„è·¯æ ‡ç‚¹çš„å€¼
            # print(f"ğŸ•µï¸â€ ã€Backendã€‘: ä¼˜åŒ–å™¨å³å°†å¤„ç†æ–°è·¯æ ‡ç‚¹ L{lm_id}ï¼Œå…¶ä¸‰è§’åŒ–åˆå§‹å€¼ä¸º: {lm_3d_pos}")
            
            # å¢åŠ ä¸€ä¸ªNaN/Infçš„æ˜¾å¼æ£€æŸ¥ï¼Œè¿™å¯¹äºè°ƒè¯•å´©æºƒè‡³å…³é‡è¦
            if np.isnan(lm_3d_pos).any() or np.isinf(lm_3d_pos).any():
                print(f"ğŸ”¥ ã€Backendã€‘[è‡´å‘½è­¦å‘Š]: è·¯æ ‡ç‚¹ L{lm_id} çš„åˆå§‹å€¼æ— æ•ˆ (NaN/Inf)ï¼ä¼˜åŒ–å³å°†å› æ­¤å´©æºƒï¼")
            # ---!!!--- æ—¥å¿—æ·»åŠ ç»“æŸ ---!!!---

            # æ£€æŸ¥ï¼š1) ä¸åœ¨æ—§å›¾ä¸­ï¼Œ2) è¿˜æ²¡è¢«æ·»åŠ è¿‡ ç¡®ä¿é¡¶ç‚¹åªè¢«æ·»åŠ ä¸€æ¬¡
            if not self.smoother.calculateEstimate().exists(L(lm_gtsam_id)):
                new_estimates.insert(L(lm_gtsam_id), lm_3d_pos)
        
        # æ·»åŠ é‡æŠ•å½±å› å­ï¼Œå‰é¢å·²ç»æ·»åŠ äº†æ–°è·¯æ ‡ç‚¹é¡¶ç‚¹ï¼Œæ‰€ä»¥è¿™é‡Œåªéœ€è¦æ·»åŠ å†å²ç‚¹å’Œæ–°ç‰¹å¾ç‚¹çš„è§‚æµ‹å¸§é‡æŠ•å½±å› å­
        current_isam_values = self.smoother.calculateEstimate()
        for kf_id, lm_id, pt_2d in new_visual_factors:
            kf_gtsam_id = self._get_kf_gtsam_id(kf_id)
            lm_gtsam_id = self._get_lm_gtsam_id(lm_id)

            kf_exists = current_isam_values.exists(X(kf_gtsam_id)) or new_estimates.exists(X(kf_gtsam_id))
            lm_exists = current_isam_values.exists(L(lm_gtsam_id)) or new_estimates.exists(L(lm_gtsam_id))

            if kf_exists and lm_exists:
                factor = gtsam.GenericProjectionFactorCal3_S2(pt_2d, self.visual_robust_noise, X(kf_gtsam_id), L(lm_gtsam_id), self.K, body_P_sensor=self.body_T_cam)
                new_graph.add(factor)
                new_window_stamps.insert((L(lm_gtsam_id), float(kf_gtsam_id))) # è¿™é‡Œä¹Ÿéœ€è¦æ›´æ–°å†å²è·¯æ ‡ç‚¹çš„æ»‘çª—è®°å½•


        # ======================= ZERO-VELOCITY UPDATE (ZUPT) - SOFT CONSTRAINT =======================
        if is_stationary:
            kf_gtsam_id = self._get_kf_gtsam_id(new_keyframe.get_id())
            zero_velocity_noise = gtsam.noiseModel.Isotropic.Sigma(3, 1e-3)
            zero_velocity_prior = gtsam.PriorFactorVector(V(kf_gtsam_id), np.zeros(3), zero_velocity_noise)
            new_graph.add(zero_velocity_prior)
            print("ã€Backendã€‘: Added Zero-Velocity-Update (ZUPT) factor.")
            # # ä½¿ç”¨BetweenFactoråœ¨è¿ç»­ä¸¤å¸§çš„é€Ÿåº¦ä¹‹é—´æ·»åŠ ä¸€ä¸ªè½¯çº¦æŸï¼Œä½¿å®ƒä»¬è¶‹äºä¸€è‡´ï¼ˆå³é€Ÿåº¦å˜åŒ–ä¸ºé›¶ï¼‰
            # last_kf_gtsam_id = self._get_kf_gtsam_id(last_keyframe.get_id())
            # new_kf_gtsam_id = self._get_kf_gtsam_id(new_keyframe.get_id())

            # # å™ªå£°æ¨¡å‹ç›¸å¯¹å®½æ¾ï¼Œå…è®¸ä¸€å®šçš„æŠ–åŠ¨
            # zero_velocity_diff_noise = gtsam.noiseModel.Isotropic.Sigma(3, 0.1) 
            
            # # çº¦æŸ V(new) - V(last) = 0
            # zupt_factor = gtsam.BetweenFactorVector(V(last_kf_gtsam_id), 
            #                                         V(new_kf_gtsam_id), 
            #                                         np.zeros(3), 
            #                                         zero_velocity_diff_noise)
            # new_graph.add(zupt_factor)
            # print("ã€Backendã€‘: Added soft Zero-Velocity-Update (ZUPT) factor between frames.")
        # ============================================================================================

        # æ‰§è¡ŒiSAM2å¢é‡æ›´æ–°
        print(f"ã€Backendã€‘: Updating iSAM2 ({new_graph.size()} new factors, {new_estimates.size()} new variables)...")
        
        try:
            start_time = time.time()
            self.smoother.update(new_graph, new_estimates, new_window_stamps)
            end_time = time.time()
            print(f"ã€Backend Timerã€‘: Incremental optimization took { (end_time - start_time) * 1000:.3f} ms.")

        except RuntimeError as e:
            print("\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
            print("!!!!!!!!!! OPTIMIZATION FAILED !!!!!!!!!!!!!!")
            print(f"ERROR: {e}")
            return

        # æ›´æ–°æœ€æ–°bias
        latest_pose, latest_vel, latest_bias = self.get_latest_optimized_state()
        print(f"ã€Backendã€‘: Latest optimized state: pose: {latest_pose.matrix()}, velocity: {latest_vel}, bias: {latest_bias}")
        latest_gtsam_id = self.next_gtsam_kf_id - 1
        if latest_bias is not None:
            self.latest_bias = latest_bias

        # è®°å½•ä¼˜åŒ–è¯¯å·®
        new_factors_error = self._log_optimization_error(new_graph)
        self._log_state_and_errors(latest_gtsam_id, latest_pose, latest_vel, latest_bias, new_factors_error)

        print("ã€Backendã€‘: Incremental optimization complete.")


    def _log_optimization_error(self, new_factors_graph):
        try:
            optimized_result = self.smoother.calculateEstimate()
            new_factors_error = new_factors_graph.error(optimized_result)

            current_full_graph = self.smoother.getFactors()

            print(f"ã€Backendã€‘ä¼˜åŒ–è¯¯å·®ç»Ÿè®¡: "
                  f"æœ¬è½®æ–°å¢å› å­è¯¯å·® = {new_factors_error:.4f}")

            # ======================= DETAILED FACTOR ERROR LOGGING =======================
            debug_start_frame = 600 # è®¾ä¸º0ä»¥ç«‹å³å¼€å§‹æ‰“å°
            latest_gtsam_id = self.next_gtsam_kf_id - 1
            if latest_gtsam_id >= debug_start_frame:
                print("\n" + "="*40 + f" DETAILED ERROR ANALYSIS (Frame {latest_gtsam_id}) " + "="*40)
                
                # éå†å›¾ä¸­çš„æ‰€æœ‰å› å­
                for i in range(current_full_graph.size()):
                    factor = current_full_graph.at(i)
                    if factor is None: # æ£€æŸ¥å› å­æ˜¯å¦æœ‰æ•ˆ
                        continue
                        
                    try:
                        # è®¡ç®—è¿™ä¸ªç‰¹å®šå› å­çš„è¯¯å·®
                        error = factor.error(optimized_result)
                        
                        # æ‰“å°è¯¯å·®å¤§äºé˜ˆå€¼çš„å› å­ï¼Œä»¥é¿å…æ—¥å¿—åˆ·å±
                        if error > 100.0: 
                            # æ‰“å°å› å­çš„Pythonç±»å
                            factor_type = factor.__class__.__name__
                            print(f"  - Factor {i}: Error = {error:.4f}, Type = {factor_type}")
                            
                            # å°è¯•æ‰“å°ä¸è¯¥å› å­ç›¸å…³çš„Key
                            keys = factor.keys()
                            key_str = ", ".join([gtsam.DefaultKeyFormatter(key) for key in keys])
                            print(f"    Keys: [{key_str}]")
                            
                    except Exception as e_factor:
                        # æ•è·è®¡ç®—å•ä¸ªå› å­è¯¯å·®æ—¶å¯èƒ½å‘ç”Ÿçš„é”™è¯¯
                        print(f"  - Factor {i}: æ— æ³•è®¡ç®—è¯¯å·®æˆ–è·å–Keys. Error: {e_factor}")

                print("="*100 + "\n")
            # ===========================================================================
            
            return new_factors_error
            
        except Exception as e:
            print(f"[Error][Backend] è®¡ç®—ä¼˜åŒ–è¯¯å·®æ—¶å‡ºé”™: {e}")
            return -1.0, -1.0
        
    def _log_state_and_errors(self, latest_gtsam_id, latest_pose, latest_vel, latest_bias, new_factors_error):
        position = latest_pose.translation()
        acc_bias = latest_bias.accelerometer()
        gyro_bias = latest_bias.gyroscope()

        state_data = {
            "gtsam_id": latest_gtsam_id,
            "pos_x": position[0], "pos_y": position[1], "pos_z": position[2],
            "vel_x": latest_vel[0], "vel_y": latest_vel[1], "vel_z": latest_vel[2],
            "bias_acc_x": acc_bias[0], "bias_acc_y": acc_bias[1], "bias_acc_z": acc_bias[2],
            "bias_gyro_x": gyro_bias[0], "bias_gyro_y": gyro_bias[1], "bias_gyro_z": gyro_bias[2],
            "new_factors_error": new_factors_error
        }
        self.logger.log_state(state_data)
